{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Problem 1 (MDP algorithms: tabular Q-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OpenAI gym FrozenLake environment*\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \n",
    "    FrozenLake-v0 defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.10.20 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.18<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">pretty-cherry-3385</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ieor-4575/lab1\" target=\"_blank\">https://wandb.ai/ieor-4575/lab1</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ieor-4575/lab1/runs/3hx9cc0y\" target=\"_blank\">https://wandb.ai/ieor-4575/lab1/runs/3hx9cc0y</a><br/>\n",
       "                Run data is saved locally in <code>/Users/chenchenwei/Documents/IEOR4575_Labs/labs/lab1/wandb/run-20210225_201452-3hx9cc0y</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wandb set up for logging runs online and moving them to the leaderboard\n",
    "import wandb\n",
    "wandb.login()\n",
    "run=wandb.init(project=\"lab1\", tags=[\"problem1\"], entity=\"ieor-4575\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CELL\n",
    "import numpy as np\n",
    "import gym\n",
    "env=gym.make('FrozenLake-v0')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For proper accounting rewards while you learn, we build a wrapper around env.step() and env.reset(). In an episode, every time you take an action the reward will be appended to the reward of the episode, and when ever the environment is reset (at the end of an epsiode), the episode reward is reset to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT CHANGE THIS CELL\n",
    "#wrapper for accounting rewards\n",
    "rEpisode=0\n",
    "rList=[]\n",
    "fixedWindow=100\n",
    "movingAverage=0\n",
    "\n",
    "def reset_decorate(func):\n",
    "    def func_wrapper():\n",
    "        global rList\n",
    "        global movingAverage\n",
    "        global rEpisode\n",
    "        global fixedwindow\n",
    "        rList.append(rEpisode)\n",
    "        if len(rList) >= fixedWindow:\n",
    "            movingAverage=np.mean(rList[len(rList)-fixedWindow:len(rList)-1])\n",
    "        rEpisode=0\n",
    "        return(func())\n",
    "    return func_wrapper\n",
    "\n",
    "env.reset = reset_decorate(env.reset)\n",
    "\n",
    "def step_decorate(func):\n",
    "    def func_wrapper(action):\n",
    "        global rEpisode\n",
    "        s1, r, d, other = func(action)\n",
    "        rEpisode+=r\n",
    "        return(s1, r, d, other)\n",
    "    return func_wrapper\n",
    "\n",
    "env.step = step_decorate(env.step)\n",
    "\n",
    "def init():\n",
    "    rEpisode=0\n",
    "    rList=[]\n",
    "    movingAverage=0\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we illustrate the execution of the Open AI gym enviornment using the policy of chosing random action in every state. Every time an action is taken the enviorment returns a tuple containing next state, reward, and the status (whether terminal state is reached or not). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RANDOM SAMPLING EXAMPLE\n",
    "num_episodes=3000; #number of episodes you want to try\n",
    "episode_max_length=100; #you can explicitly end the epsiode before terminal state is reached\n",
    "\n",
    "env.reset()\n",
    "#env.render()\n",
    "#execute in episodes\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    #reset the environment at the beginning of an episode\n",
    "    s = env.reset()\n",
    "    d = False #not done\n",
    "    \n",
    "    for t in range(episode_max_length):\n",
    "        \n",
    "        ################ Random action policy ###########################\n",
    "        #play random action \n",
    "        a = env.action_space.sample()\n",
    "        #get new state, reward, done\n",
    "        s, r, d, _ = env.step(a)\n",
    "        #################################################################\n",
    "        \n",
    "        \n",
    "        #break if done, reached terminal state \n",
    "        if d == True:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    #log per-episode reward and moving average over 100 episodes\n",
    "    wandb.log({ \"random reward\" : rEpisode, \"random reward moving average\" : movingAverage, \"random episode\" : i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement tabular Q-learning (*YOU SHOULD ONLY CHANGE THE CELL BELOW*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --------- Avg reward per 1000 episodes --------- \n",
      "\n",
      "Iter  1000 :  0.17200000000000013\n",
      "Iter  2000 :  0.4930000000000004\n",
      "Iter  3000 :  0.6250000000000004\n",
      "Iter  4000 :  0.5880000000000004\n",
      "Iter  5000 :  0.6590000000000005\n",
      "Iter  6000 :  0.6010000000000004\n",
      "Iter  7000 :  0.6510000000000005\n",
      "Iter  8000 :  0.6150000000000004\n",
      "Iter  9000 :  0.6360000000000005\n",
      "Iter  10000 :  0.5900000000000004\n",
      "Iter  11000 :  0.6430000000000005\n",
      "Iter  12000 :  0.6650000000000005\n",
      "Iter  13000 :  0.6690000000000005\n",
      "Iter  14000 :  0.5810000000000004\n",
      "Iter  15000 :  0.6020000000000004\n",
      "Iter  16000 :  0.5850000000000004\n",
      "Iter  17000 :  0.5720000000000004\n",
      "Iter  18000 :  0.5570000000000004\n",
      "Iter  19000 :  0.6180000000000004\n",
      "Iter  20000 :  0.5870000000000004\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "#initialize episodic structure\n",
    "init()\n",
    "num_episodes=20000; #number of training episodes, you can increase this to train more\n",
    "episode_max_length=100;\n",
    "\n",
    "np.random.seed(256) # random stay same for tunning param \n",
    "\n",
    "#initialize discount factor, learning rate\n",
    "gamma=0.9\n",
    "# Best performance when gamma = 0.95\n",
    "\n",
    "learnRate=0.63\n",
    "\n",
    "# 0.7 --> 0.667\n",
    "# 0.65 --> 0.7677 \n",
    "# 0.64 --> 0.434  \n",
    "# 0.63 --> 0.7778 \n",
    "# 0.625 --> 0.5859 \n",
    "# 0.6 --> 0.727  \n",
    "\n",
    "\n",
    "\n",
    "#create Q table\n",
    "Q=np.zeros([env.observation_space.n, env.action_space.n]) #Q(s,a). The Q-values from this array will be used to evaluate your policy.\n",
    "n=np.ones([env.observation_space.n, env.action_space.n])  #recording number of trails for each arm\n",
    "n_actions = env.action_space.n\n",
    "rewards_history = [] \n",
    "\n",
    "explor_rate = 1.0\n",
    "min_explor_rate = 0.001  \n",
    "max_explor_rate = 1.0\n",
    "explor_decay_rate = 0.005 # 0.005 is reasonable from experiment \n",
    "\n",
    "#execute in episodes\n",
    "for i in range(num_episodes):\n",
    "    #reset the environment at the beginning of an episode\n",
    "    s = env.reset()\n",
    "    d = False #not done\n",
    "    episode_rewards = 0\n",
    "    for j in range(episode_max_length):\n",
    "        \n",
    "        ###########SELCT ACTION a for state s using Q-values ##################\n",
    "        #example\n",
    "        #a = np.argmax(Q[s,:])\n",
    "        #a = env.action_space.sample()\n",
    "         \n",
    "        random_num = random.uniform(0, 1) \n",
    "        if random_num <= explor_rate:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(Q[s,:]) \n",
    "         \n",
    "        #get new state, reward, done\n",
    "        s1, r, d,_ = env.step(a)\n",
    "        \n",
    "        #update Q(s,a)\n",
    "        Q[s,a] = (1 - learnRate) * Q[s,a] + learnRate * (r + gamma*np.max(Q[s1,:]))\n",
    "        episode_rewards += r \n",
    "    \n",
    "        #break if done, reached terminal state \n",
    "        if d == True:\n",
    "            break\n",
    "        s=s1\n",
    "        \n",
    "    explor_rate = min_explor_rate + \\\n",
    "    (max_explor_rate - min_explor_rate) * np.exp(-explor_decay_rate*(i))\n",
    "    \n",
    "    rewards_history.append(episode_rewards)\n",
    "    #log per-episode reward and moving average over 100 episodes\n",
    "    wandb.log({ \"training reward\" : rEpisode, \"training reward moving average\" : movingAverage, \"training episode\" : i})\n",
    "wandb.run.summary[\"number of training episodes\"]=num_episodes\n",
    "\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_history),num_episodes/1000) \n",
    "count = 1000 \n",
    "print(\" --------- Avg reward per 1000 episodes --------- \\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(\"Iter \", count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/ieor-4575/lab1/runs/3hx9cc0y?jupyter=true\" style=\"border:none;width:100%;height:420px\">\n",
       "                </iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.Run at 0x1112a6990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%wandb\n",
    "## DO NOT CHANGE THIS CELL. CHANGING ANY PART OF THIS CELL CAN DISQUALIFY THE SUBMISSION\n",
    "#Evaluation of trained policy\n",
    "init()\n",
    "num_episodes=1000; #number of episodes for evaluation\n",
    "episode_max_length=100; \n",
    "movingAverageArray=[]\n",
    "score=0\n",
    "env.reset()\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    d = False #not done\n",
    "    for t in range(episode_max_length):\n",
    "        a = np.argmax(Q[s,:])\n",
    "        s, r, d, _ = env.step(a)\n",
    "        if d == True:\n",
    "            break\n",
    "    #log per-episode reward and moving average over 100 episodes\n",
    "    wandb.log({ \"evaluation reward\" : rEpisode, \"evaluation reward moving average\" : movingAverage, \"evaluation episode\" : i})\n",
    "    movingAverageArray.append(movingAverage)\n",
    "    #score is x if there is a window of 100 consecutive episodes where moving average was at least x\n",
    "    if i>100:\n",
    "        score=max(score,min(movingAverageArray[i-100:i-1]))\n",
    "\n",
    "\n",
    "wandb.run.summary[\"score\"]=score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 93367<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db9dbf8604343e68c57939ffe4016a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/chenchenwei/Documents/IEOR4575_Labs/labs/lab1/wandb/run-20210225_201452-3hx9cc0y/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/chenchenwei/Documents/IEOR4575_Labs/labs/lab1/wandb/run-20210225_201452-3hx9cc0y/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>random reward</td><td>0.0</td></tr><tr><td>random reward moving average</td><td>0.0101</td></tr><tr><td>random episode</td><td>2999</td></tr><tr><td>_runtime</td><td>31</td></tr><tr><td>_timestamp</td><td>1614302123</td></tr><tr><td>_step</td><td>23999</td></tr><tr><td>training reward</td><td>1.0</td></tr><tr><td>training reward moving average</td><td>0.73737</td></tr><tr><td>training episode</td><td>19999</td></tr><tr><td>number of training episodes</td><td>20000</td></tr><tr><td>evaluation reward</td><td>1.0</td></tr><tr><td>evaluation reward moving average</td><td>0.70707</td></tr><tr><td>evaluation episode</td><td>999</td></tr><tr><td>score</td><td>0.75758</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>random reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>random reward moving average</td><td>▁▁▃▅▃▃▅▃▅██▅▅▁▃▅▃▆▆▃▅██▅▁▁▃▅▃▃▅▅▁▁▁▃▅▃▃▃</td></tr><tr><td>random episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training reward</td><td>▁▁████▁█▁█▁▁███▁███████████▁███▁█▁█████▁</td></tr><tr><td>training reward moving average</td><td>▁▂▄▆▅▇▆▄▇▇▆▇▇▇▇▇▇▆▄▇▆▆█▇▇▆▆▄▅▇▅▅▇▆▆▅▇█▇█</td></tr><tr><td>training episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>evaluation reward</td><td>▁█████████▁▁██▁█▁██▁▁▁█▁███▁▁█▁▁███▁████</td></tr><tr><td>evaluation reward moving average</td><td>▅▅▅▇▇▇▆▆▇███▆▅▅▅▅▅▆▅█▇▄▃▁▃▃▄▅▂▄▅▄▆▅▅▃▃▃▄</td></tr><tr><td>evaluation episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">pretty-cherry-3385</strong>: <a href=\"https://wandb.ai/ieor-4575/lab1/runs/3hx9cc0y\" target=\"_blank\">https://wandb.ai/ieor-4575/lab1/runs/3hx9cc0y</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieor4575",
   "language": "python",
   "name": "ieor4575"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}