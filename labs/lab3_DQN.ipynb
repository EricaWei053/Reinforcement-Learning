{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.18<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">summer-rain-2720</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ieor-4575/lab3\" target=\"_blank\">https://wandb.ai/ieor-4575/lab3</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ieor-4575/lab3/runs/3jm8jmdu\" target=\"_blank\">https://wandb.ai/ieor-4575/lab3/runs/3jm8jmdu</a><br/>\n",
       "                Run data is saved locally in <code>/Users/chenchenwei/Documents/IEOR4575_Labs/labs/lab3_DQN/torch/wandb/run-20210427_145735-3jm8jmdu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "run=wandb.init(project=\"lab3\", entity=\"ieor-4575\", tags=[\"torch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN (Deep Q Network)\n",
    "\n",
    "In previous Labs, we have learned to use Pytorch to build deep learning models. In this lab, we will apply deep learning as function approximations in reinforcement learning. \n",
    "\n",
    "Reference: DQN https://arxiv.org/abs/1312.5602\n",
    "\n",
    "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
    "\n",
    "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*there are alternative parameterizations here*). Let $Q_\\theta(s,a)$ be the output of the network, to estimate the optimal action value function in state $s$ and take action $a$ (and follow optimal policy thereafter). \n",
    "\n",
    "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman optimality equation\n",
    "\n",
    "We will use Bellman optimality equation to find $\\theta$ such that the above approximation holds better. Recall that for optimal Q function $Q^\\ast(s,a)$ the following holds for all $(s,a)$\n",
    "\n",
    "$$Q^\\ast(s_t,a_t) = \\mathbb{E}\\big[r_t + \\gamma \\max_a Q^\\ast(s_{t+1},a)\\big]$$\n",
    "\n",
    "where the expectation is wrt the random reward $r_t$ and transition to the next state $s_{t+1}$. A natural objective to consider is \n",
    "\n",
    "$$\\min_\\theta\\  (Q_\\theta(s_t,a_t) -\\mathbb{E}\\big[r_t + \\gamma  \\max_a  Q_{\\hat \\theta}(s_{t+1},a)\\big])^2$$\n",
    "at the current or previous $\\hat \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the DQN model \n",
    "\n",
    "The first step is to build a neural network with parameters $\\theta$ that predicts $Q_\\theta(s,a)$ for any $(s,a)$. You can either build a network that \n",
    "\n",
    "* (in case of small number $K$ of discrete actions) takes as input a  representation of state $s$ and outputs a $K$-dimensional vector giving scores $Q(s,a), a=1,\\ldots, K$ for all actions\n",
    "\n",
    "or \n",
    "\n",
    "* takes as input a concatenated representation of state and action $(s,a)$ and output one dimensional score $Q_\\theta(s,a)$,\n",
    "\n",
    "Below we have provided a skeleton code (incomplete) for defining and training the Q-function. **You need to fill in the DNN model definition and loss function definition**. Refer to regression lab (lab 2) for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural net Q_\\theta(s,a) as a class\n",
    "\n",
    "class Qfunction(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, lr):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        sess: sess to execute this Qfunction\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        # DEFINE THE MODEL\n",
    "        self.model = torch.nn.Sequential(\n",
    "                    #TODO      \n",
    "                    #input layer\n",
    "                    torch.nn.Linear(obssize, 128),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(128, 256),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(256, actsize)\n",
    "                )\n",
    "        \n",
    "        # DEFINE THE OPTIMIZER\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # RECORD HYPER-PARAMS\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "        \n",
    "    def _to_one_hot(self, y, num_classes):\n",
    "        \"\"\"\n",
    "        convert an integer vector y into one-hot representation\n",
    "        \"\"\"\n",
    "        scatter_dim = len(y.size())\n",
    "        y_tensor = y.view(*y.size(), -1)\n",
    "        zeros = torch.zeros(*y.size(), num_classes, dtype=y.dtype)\n",
    "        return zeros.scatter(scatter_dim, y_tensor, 1)\n",
    "\n",
    "    \n",
    "    def compute_Qvalues(self, states, actions):\n",
    "        \"\"\"\n",
    "        input: list of numsamples state-action pairs\n",
    "        output: List of Q values for each input (s,a). The output will have size [numsamples, 1] \n",
    "        \"\"\"\n",
    "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
    "        #This will be different for neural network that takes as input a state-action pair\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        q_preds = self.model(states)\n",
    "        action_onehot = self._to_one_hot(actions, actsize)\n",
    "        q_preds_selected = torch.sum(q_preds * action_onehot, axis=-1)\n",
    "\n",
    "        return q_preds_selected\n",
    "        \n",
    "    def compute_maxQvalues(self, states):\n",
    "        \"\"\"\n",
    "        input: a list of numsamples states \n",
    "        output: max_a Q(s,a) values for every input state s in states. The output will have size numsamples\n",
    "        \"\"\"\n",
    "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
    "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        Qvalues = self.model(states).cpu().data.numpy()\n",
    "        q_preds_greedy = np.max(Qvalues,1) \n",
    "        \n",
    "        return q_preds_greedy\n",
    "        \n",
    "    def compute_argmaxQ(self, state):\n",
    "        \"\"\"\n",
    "        input: one state s\n",
    "        output: arg max_a Q(self.model(states).cpu().data.numpy()s,a) values for the input state s. The output will have size 1\n",
    "        \"\"\"\n",
    "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
    "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        Qvalue = self.model(state).cpu().data.numpy()\n",
    "        greedy_action = np.argmax(Qvalue.flatten())\n",
    "        \n",
    "        return greedy_action\n",
    "        \n",
    "        \n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        targets = torch.FloatTensor(targets)\n",
    "        \n",
    "        # COMPUTE Q PREDICTIONS for all state-action pairs\n",
    "        q_preds_selected = self.compute_Qvalues(states, actions)\n",
    "\n",
    "                \n",
    "        # LOSS\n",
    "        #print(q_preds_selected.shape, targets.shape)\n",
    "        loss = torch.mean((q_preds_selected - targets)**2)\n",
    "\n",
    "        # BACKWARD PASS\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return loss.detach().cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can skip ahead to implementing the basic Q-learning that at every step $t$ in the environment\n",
    "* given state $s_t$, computes greedy actions from Q-values (using compute_argmaxQ function above) and uses $\\epsilon$-greedy select an action $a_t$, \n",
    "* makes observation of reward $r_t$ and next state $s_{t+1}$\n",
    "* using compute_maxQvalues() function, computes target\n",
    "  $$r_t + \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
    "and then retrains the Q-function using train() function above (with numsamples=1)\n",
    "\n",
    "However, for improved performance you may want to consider ideas like batch training (numsamples>1 is the batch size) with experience replay buffer and target-networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replay Buffer**\n",
    "\n",
    "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$, when we minimize the Bellman error. When optimizing the Bellman error loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples from buffer $(s_i,a_i,r_i,s_{i}') \\sim R$ and then compute\n",
    "targets\n",
    "\n",
    "$$d_i=r_i + \\max_a \\gamma Q_{\\theta}(s_i^\\prime,a)$$\n",
    "for all $i$. Use the above training function train() with input as list $(s_i, a_i, d_i)_{i=1}^N$  to update parameters using backprop.\n",
    "\n",
    "**Target Network**\n",
    "\n",
    "Maintain a target network in addition to the original pricipal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^-$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
    "\n",
    "$$d_i =  r_t + \\gamma \\max_a Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
    "\n",
    "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence several critical parts of the complete pseudocode for DQN is as follows:\n",
    "\n",
    "**Initialization.**\n",
    "principal network $Q_\\theta(s,a)$, target network $Q_{\\theta^{-}}(s,a)$. Replay buffer $R = \\{\\}$ (empty). \n",
    "\n",
    "**At each time step $t.$**\n",
    "The agent executes action using $\\epsilon-$greedy based on the principal network $Q_\\theta(s,a)$. To update $\\theta$: sample $N$ tuples $(s_i,a_i,r_i,s_i^\\prime) \\sim R$, compute empirical loss \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - (r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a))^2$$\n",
    "\n",
    "and update parameter $\\theta$ using backprop (just take one gradient step).\n",
    "\n",
    "**Update target network.**\n",
    "Every $\\tau$ time steps, update target network by copying $\\theta_{\\text{target}} \\leftarrow \\theta$.\n",
    "\n",
    "**Bellman target.**\n",
    "Above, we have defined the target values as being computed from a target net with parameter $\\theta^-$ \n",
    "$$r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a)$$\n",
    "It is worth thinking about what happens if we are at the end of an episode, that is, what if $s_i^\\prime$ here is a terminal state. In this case, should the Bellman error be defined exactly the same as above? Do we need some modifications? Think carefully about this as this will greatly impact the algorithmic performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement replay buffer\n",
    "import random\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        minibatch = random.sample(self.buffer,batchsize)\n",
    "        return minibatch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code snippet for copying target network\n",
    "You may use th following to update target network i.e. to copy from principal network to target network. We need to use tensorflow scope to distinguish the computational graphs of target and principal networks. The following function builds a tensorflow operation that does the copying $\\theta^- \\leftarrow \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_target_update(Qprincipal, Qtarget):\n",
    "    for v,v_ in zip(Qprincipal.model.parameters(), Qtarget.model.parameters()):\n",
    "        v_.data.copy_(v.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for DQN \n",
    "Now that we have all the ingredients for DQN, we can write the main procedure to train DQN on a given environment. The implementation is straightforward if you follow the pseudocode pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/ieor-4575/lab3/runs/3jm8jmdu?jupyter=true\" style=\"border:none;width:100%;height:420px\">\n",
       "                </iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.Run at 0x12fe14d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffersize 27\n",
      "episode 0 ave training returns 27.0\n",
      "buffersize 319\n",
      "episode 10 ave training returns 29.2\n",
      "buffersize 500\n",
      "episode 20 ave training returns 24.7\n",
      "buffersize 500\n",
      "episode 30 ave training returns 11.8\n",
      "buffersize 500\n",
      "episode 40 ave training returns 16.9\n",
      "buffersize 500\n",
      "episode 50 ave training returns 17.4\n",
      "buffersize 500\n",
      "episode 60 ave training returns 18.7\n",
      "buffersize 500\n",
      "episode 70 ave training returns 16.8\n",
      "buffersize 500\n",
      "episode 80 ave training returns 23.2\n",
      "buffersize 500\n",
      "episode 90 ave training returns 18.8\n",
      "buffersize 500\n",
      "episode 100 ave training returns 19.7\n",
      "buffersize 500\n",
      "episode 110 ave training returns 9.2\n",
      "buffersize 500\n",
      "episode 120 ave training returns 9.5\n",
      "buffersize 500\n",
      "episode 130 ave training returns 9.6\n",
      "buffersize 500\n",
      "episode 140 ave training returns 9.9\n",
      "buffersize 500\n",
      "episode 150 ave training returns 15.2\n",
      "buffersize 500\n",
      "episode 160 ave training returns 20.0\n",
      "buffersize 500\n",
      "episode 170 ave training returns 38.5\n",
      "buffersize 500\n",
      "episode 180 ave training returns 123.3\n",
      "buffersize 500\n",
      "episode 190 ave training returns 191.4\n",
      "buffersize 500\n",
      "episode 200 ave training returns 173.6\n",
      "buffersize 500\n",
      "episode 210 ave training returns 167.7\n",
      "buffersize 500\n",
      "episode 220 ave training returns 155.4\n",
      "buffersize 500\n",
      "episode 230 ave training returns 176.1\n",
      "buffersize 500\n",
      "episode 240 ave training returns 187.9\n",
      "buffersize 500\n",
      "episode 250 ave training returns 179.6\n",
      "buffersize 500\n",
      "episode 260 ave training returns 144.4\n",
      "buffersize 500\n",
      "episode 270 ave training returns 198.1\n",
      "buffersize 500\n",
      "episode 280 ave training returns 162.6\n",
      "buffersize 500\n",
      "episode 290 ave training returns 146.2\n",
      "buffersize 500\n",
      "episode 300 ave training returns 200.0\n",
      "buffersize 500\n",
      "episode 310 ave training returns 200.0\n",
      "buffersize 500\n",
      "episode 320 ave training returns 68.9\n",
      "buffersize 500\n",
      "episode 330 ave training returns 176.0\n",
      "buffersize 500\n",
      "episode 340 ave training returns 199.1\n",
      "buffersize 500\n",
      "episode 350 ave training returns 178.4\n",
      "buffersize 500\n",
      "episode 360 ave training returns 196.0\n",
      "buffersize 500\n",
      "episode 370 ave training returns 200.0\n",
      "buffersize 500\n",
      "episode 380 ave training returns 151.3\n",
      "buffersize 500\n",
      "episode 390 ave training returns 181.5\n",
      "buffersize 500\n",
      "episode 400 ave training returns 193.4\n",
      "buffersize 500\n",
      "episode 410 ave training returns 158.2\n",
      "buffersize 500\n",
      "episode 420 ave training returns 186.6\n",
      "buffersize 500\n",
      "episode 430 ave training returns 200.0\n",
      "buffersize 500\n",
      "episode 440 ave training returns 184.9\n",
      "buffersize 500\n",
      "episode 450 ave training returns 170.2\n",
      "buffersize 500\n",
      "episode 460 ave training returns 151.1\n",
      "buffersize 500\n",
      "episode 470 ave training returns 168.9\n",
      "buffersize 500\n",
      "episode 480 ave training returns 134.7\n",
      "buffersize 500\n",
      "episode 490 ave training returns 171.5\n"
     ]
    }
   ],
   "source": [
    "%%wandb\n",
    "#remove above line if you do not want to see inline plots from wandb\n",
    "\n",
    "# hyper-parameters\n",
    "lr = 1e-3  # learning rate for gradient update\n",
    "batchsize = 128  # batchsize for buffer sampling\n",
    "maxlength = 500  # max number of tuples held by buffer\n",
    "envname = \"CartPole-v0\"  # environment name\n",
    "tau = 100  # time steps for target update\n",
    "episodes = 500  # number of episodes to run\n",
    "initialsize = 100  # initial time steps before start training\n",
    "epsilon = .2  # constant for exploration\n",
    "gamma = .99  # discount\n",
    "\n",
    "explore_rate = 1.0\n",
    "min_explore_rate = 0.05  \n",
    "max_explore_rate = 1.0\n",
    "explore_decay_rate = 0.005\n",
    "\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "# initialize Q-function networks (princpal and target)\n",
    "Qprincipal = Qfunction(obssize, actsize, lr)\n",
    "Qtarget = Qfunction(obssize, actsize, lr)\n",
    "\n",
    "# initialization of graph and buffer\n",
    "buffer = ReplayBuffer(maxlength)\n",
    "\n",
    "# main iteration\n",
    "rrecord = []\n",
    "totalstep = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rsum = 0 \n",
    "    while not done:\n",
    "        \n",
    "        explore_rate = min_explore_rate + \\\n",
    "        (max_explore_rate - min_explore_rate) * np.exp(-explore_decay_rate*(totalstep))\n",
    "        \n",
    "        # epsilon greedy for exploration\n",
    "        random_num = random.uniform(0, 1) \n",
    "        if random_num <= explore_rate:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = Qprincipal.compute_argmaxQ(np.expand_dims(obs,0))\n",
    "      \n",
    "        newobs, r, done, _ = env.step(action)\n",
    "        done_ = 1 if done else 0\n",
    "        e = (obs, action, r, done_, newobs) \n",
    "        # e = (array([ 0.0297525 ,  0.0352823 , -0.03689473, -0.03157034]), \n",
    "        # 1, 1.0, 0, array([ 0.03045814,  0.23091338, -0.03752614, -0.33566185]))\n",
    "     \n",
    "        \n",
    "        #IF NOT USING BUFFER:\n",
    "        #use single sample (obs, action, r, done_, newobs) with Qtarget to compute target and train Qprincipal\n",
    "        \n",
    "        # ELSE IF USING REPLAY BUFFER\n",
    "        # append experiences e to buffer\n",
    "        \n",
    "        buffer.append(e)\n",
    "        while buffer.number > maxlength:\n",
    "            buffer.pop()\n",
    "            \n",
    "    \n",
    "        # every few episodes (decide the frequency) sample a minibatch from buffer\n",
    "        # compute targets in batch using Qtarget and train Qprincipal \n",
    "        if episode >= initialsize:\n",
    "            if buffer.number >= maxlength:\n",
    "                batch_sample = buffer.sample(batchsize)\n",
    "\n",
    "                b_states =  list(np.array(batch_sample)[:, 0])\n",
    "                b_actions = list(np.array(batch_sample)[:, 1])\n",
    "                b_reward  = list(np.array(batch_sample)[:, 2])\n",
    "                b_next_state = list(np.array(batch_sample)[:, 4])\n",
    "                b_done = list(np.array(batch_sample)[:, 3])\n",
    "                \n",
    "                next_state_q_vals = np.zeros(batchsize)\n",
    "                next_state_q_vals = Qtarget.compute_maxQvalues(b_next_state)\n",
    "                \n",
    "                # deal with done states \n",
    "                for idx in range(batchsize):\n",
    "                    if b_done[idx]:\n",
    "                        next_state_q_vals[idx] = b_reward[idx]\n",
    "                     \n",
    "                #targets = Qtarget.compute_maxQvalues(b_states)\n",
    "                targets = b_reward + gamma * next_state_q_vals\n",
    "\n",
    "                Qprincipal.train(b_states, b_actions, targets)\n",
    "       \n",
    "        #UPDATE target network \n",
    "        #every tau steps update copy the principal network to the target network\n",
    "        if totalstep % tau == 0:\n",
    "            run_target_update(Qprincipal, Qtarget)\n",
    " \n",
    "        # update\n",
    "        totalstep += 1\n",
    "        rsum += r\n",
    "        obs = newobs \n",
    "        if done_: break \n",
    "    \n",
    "        \n",
    "\n",
    "    #The code below is for printing and debugging at the end of episode\n",
    "    rrecord.append(rsum)\n",
    "    \n",
    "    # printing functions for debugging purposes. Feel free to add more\n",
    "    if episode % 10 == 0:\n",
    "        print('buffersize {}'.format(buffer.number))\n",
    "        print('episode {} ave training returns {}'.format(episode, np.mean(rrecord[-10:])))\n",
    "    \n",
    "    #printing moving averages for smoothed visualization. \n",
    "    fixedWindow=100\n",
    "    movingAverage=0\n",
    "    if len(rrecord) >= fixedWindow:\n",
    "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
    "    wandb.log({ \"training reward\" : rsum, \"train reward moving average\" : movingAverage})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance of the trained agent. We will evaluate the performance of the greedy policy wrt learned Q-function. The evaluation will be run 10 times, each for eval_epsiodes and print out the average performance across these episodes. Please **do not** change the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT CHANGE\n",
    "def evaluate(Q, env, episodes):\n",
    "    # main iteration\n",
    "    score = 0.0\n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        rsum = 0\n",
    "        \n",
    "        while not done:\n",
    "            # always greedy\n",
    "            action = Q.compute_argmaxQ(np.expand_dims(obs,0))\n",
    "            \n",
    "\n",
    "            # mdp stepping forward\n",
    "            newobs, r, done, _ = env.step(action)\n",
    "\n",
    "            # update data\n",
    "            rsum += r\n",
    "            obs = newobs        \n",
    "\n",
    "        \n",
    "        wandb.log({\"eval reward\" : rsum})\n",
    "        score = score + rsum\n",
    "    score = score/episodes\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE CODE HERE\n",
    "# after training, we will evaluate the performance of the agent\n",
    "# on a target environment\n",
    "env_test = gym.make(envname)\n",
    "eval_episodes = 1000\n",
    "score = evaluate(Qprincipal, env_test, eval_episodes)\n",
    "wandb.run.summary[\"score\"]=score \n",
    "\n",
    "print(\"eval performance of DQN agent: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieor4575",
   "language": "python",
   "name": "ieor4575"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
